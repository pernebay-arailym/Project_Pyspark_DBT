# ğŸ›ï¸ Sales Data Pipeline with PySpark + dbt + DuckDB

## ğŸ“Œ Project Overview
This project demonstrates the **end-to-end workflow of a modern data pipeline**:
- **Raw CSV data** (sales transactions) is cleaned and transformed using **PySpark**.
- Cleaned data is stored in a **DuckDB database**.
- A **star schema** is designed and materialized with **dbt**.
- **Analytical queries** are written in dbt to answer real business questions.

The goal is to simulate a real-world **Data Engineering pipeline** from ingestion to analytics.

---

## ğŸ—‚ï¸ Project Structure

```
Project_Pyspark_DBT/
â”œâ”€â”€ data/                   # Raw and cleaned CSV files
â”‚   â”œâ”€â”€ ventes.csv          # Original raw data
â”‚   â””â”€â”€ ventes_clean.csv    # Cleaned export from PySpark
â”œâ”€â”€ duckdb/
â”‚   â””â”€â”€ ventes_clean.duckdb # DuckDB database with star schema
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ cleaning_pyspark.ipynb  # Interactive PySpark cleaning process
â”œâ”€â”€ models/                 # dbt models
â”‚   â”œâ”€â”€ staging/            # Staging layer (stg_ventes.sql)
â”‚   â”œâ”€â”€ dimensions/         # Dimension tables
â”‚   â”œâ”€â”€ facts/              # Fact table (fact_ventes.sql)
â”‚   â””â”€â”€ queries/            # Analytical queries (business questions)
â”œâ”€â”€ myenv/                  # Python virtual environment
â”œâ”€â”€ dbt_project.yml         # dbt project configuration
â”œâ”€â”€ README.md               # â† You are here
â””â”€â”€ .gitignore
```
---
### âš™ï¸ Technologies Used
- Python 3.13
- PySpark â†’ Data cleaning & preprocessing
- DuckDB â†’ Lightweight OLAP database
- dbt (Data Build Tool) â†’ Data transformations, star schema, analytics
- VS Code + Jupyter Notebook â†’ Development environment
---
### ğŸ”„ Workflow
#### 1. Data Cleaning with PySpark
- Loaded raw ventes.csv.
- Applied data quality rules:
- Removed rows with missing values (except montant_total).
- Corrected purchase dates with multiple formats.
- Converted string columns to numeric types (age, quantity, price).
- Fixed anomalies (negative ages â†’ positive).
- Recalculated montant_total = quantite Ã— prix_unitaire.

#### â¡ï¸ Exported as data/ventes_clean.csv.
---
### 2. Star Schema with dbt

Staging model: stg_ventes.sql

Dimensions:

- dim_client.sql â†’ customer info
- dim_produit.sql â†’ product info
- dim_magasin.sql â†’ store info
- dim_temps.sql â†’ time dimension

Fact table: fact_ventes.sql

#### â¡ï¸ Built with:
```
dbt run
```
---
### 3. Business Questions (Analytical Queries)

Inside models/queries/, queries answer key business questions.

#### ğŸ“Š Temporal Analysis

-- Monthly revenue of the store
-- Top 3 most profitable months
-- Evolution of the average basket size

#### ğŸ“¦ Product Analysis

-- Best-selling products by quantity
-- Top products by revenue
-- Most popular product categories

#### ğŸ‘¥ Client Analysis

-- Sales distribution by age group (10-year ranges)
-- Cities generating the highest sales

#### â¡ï¸ Built with:
```
dbt run --select queries
```
---
### ğŸ“ˆ Example Results

- *Monthly revenue* shows seasonal trends.
- *Top 3 months* highlight peak sales periods.
- *Average basket* size grows steadily over time.
- *Top products and categories* drive the majority of revenue.
- *Client demographics* reveal most active age groups and best-performing regions.
---
### ğŸ¯ Key Learnings

- Hands-on practice with **PySpark data cleaning**.
- Implementing a **star schema** for analytics.
- Using dbt with **DuckDB for modular**, version-controlled SQL transformations.
- Writing **business-focused queries** on top of clean, modeled data.
---
### ğŸ“ Author

ğŸ‘©â€ğŸ’» ğŸ”— [Arailym Pernebay](https://www.linkedin.com/in/arailym-pernebay/)
