{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f1892b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME = None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"JAVA_HOME =\", os.environ.get(\"JAVA_HOME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36b484",
   "metadata": {},
   "source": [
    "#### Verified which Python my notebook is using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e753ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pernebayarailym/anaconda3/envs/myenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b286b32",
   "metadata": {},
   "source": [
    "#### Ensured PySpark installs in the same Python environment my notebook is using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76608d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/pernebayarailym/anaconda3/envs/myenv/lib/python3.9/site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /Users/pernebayarailym/anaconda3/envs/myenv/lib/python3.9/site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93135554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495a2d9",
   "metadata": {},
   "source": [
    "##### Used these commands to install apache-spark because it couldn't find from homebrew , so I manually set the SPARK_HOME path using the path from the command below:\n",
    "\n",
    "1) brew install apache-spark\n",
    "2) brew --prefix apache-spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e157968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/29 14:50:23 WARN Utils: Your hostname, MacBook-Air--Arailym.local, resolves to a loopback address: 127.0.0.1; using 172.20.10.2 instead (on interface en0)\n",
      "25/09/29 14:50:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/29 14:50:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp_cleaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed489a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary PySpark modules\n",
    "from pyspark.sql import SparkSession #is the entry point to PySpark\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d76b3d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transaction: string (nullable = true)\n",
      " |-- client_nom: string (nullable = true)\n",
      " |-- client_age: string (nullable = true)\n",
      " |-- client_ville: string (nullable = true)\n",
      " |-- produit_nom: string (nullable = true)\n",
      " |-- produit_categorie: string (nullable = true)\n",
      " |-- produit_marque: string (nullable = true)\n",
      " |-- prix_catalogue: string (nullable = true)\n",
      " |-- magasin_nom: string (nullable = true)\n",
      " |-- magasin_type: string (nullable = true)\n",
      " |-- magasin_region: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- quantite: string (nullable = true)\n",
      " |-- montant_total: string (nullable = true)\n",
      "\n",
      "+--------------+----------+----------+------------+----------------+-----------------+--------------+--------------+--------------+------------+--------------------+----------+--------+-------------+\n",
      "|id_transaction|client_nom|client_age|client_ville|     produit_nom|produit_categorie|produit_marque|prix_catalogue|   magasin_nom|magasin_type|      magasin_region|      date|quantite|montant_total|\n",
      "+--------------+----------+----------+------------+----------------+-----------------+--------------+--------------+--------------+------------+--------------------+----------+--------+-------------+\n",
      "|             1|     Alice|        25|       Paris|      Ordinateur|     Informatique|          Dell|           800| Boutique Lyon|    Physique|Auvergne-Rhône-Alpes|2023-03-12|       2|         NULL|\n",
      "|             2|       Bob|        34|        Lyon|      Smartphone|       Téléphonie|         Apple|          1200| Boutique Lyon|    Physique|Auvergne-Rhône-Alpes|2023-01-27|       5|         NULL|\n",
      "|             3|     Alice|        25|       Paris|Montre connectée|      Accessoires|        Garmin|           300|        E-Shop|    En ligne|            National|2023-01-09|       1|         NULL|\n",
      "|             4|     Alice|        25|       Paris|      Smartphone|       Téléphonie|         Apple|          1200|Boutique Paris|    Physique|       Île-de-France|2023-05-10|       5|         NULL|\n",
      "|             5|     Alice|        25|       Paris|Montre connectée|      Accessoires|        Garmin|           300|Boutique Paris|    Physique|       Île-de-France|2023-06-16|       5|         NULL|\n",
      "+--------------+----------+----------+------------+----------------+-----------------+--------------+--------------+--------------+------------+--------------------+----------+--------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Read CSV files\n",
    "csv_path = '/Users/pernebayarailym/Documents/Portfolio_Projects_AP/Simplon_DE_Projects/Python_Projects/Project_Pyspark_DBT/data/ventes.csv'\n",
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", False) \\\n",
    "    .csv(csv_path)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5081f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+------------+----------------+-----------------+--------------+--------------+--------------+------------+--------------------+----------+--------+-------------+\n",
      "|id_transaction|client_nom|client_age|client_ville|     produit_nom|produit_categorie|produit_marque|prix_catalogue|   magasin_nom|magasin_type|      magasin_region|      date|quantite|montant_total|\n",
      "+--------------+----------+----------+------------+----------------+-----------------+--------------+--------------+--------------+------------+--------------------+----------+--------+-------------+\n",
      "|             1|     alice|        25|       paris|      ordinateur|     informatique|          dell|           800| boutique lyon|    physique|auvergne-rhône-alpes|2023-03-12|       2|         NULL|\n",
      "|             2|       bob|        34|        lyon|      smartphone|       téléphonie|         apple|          1200| boutique lyon|    physique|auvergne-rhône-alpes|2023-01-27|       5|         NULL|\n",
      "|             3|     alice|        25|       paris|montre connectée|      accessoires|        garmin|           300|        e-shop|    en ligne|            national|2023-01-09|       1|         NULL|\n",
      "|             4|     alice|        25|       paris|      smartphone|       téléphonie|         apple|          1200|boutique paris|    physique|       île-de-france|2023-05-10|       5|         NULL|\n",
      "|             5|     alice|        25|       paris|montre connectée|      accessoires|        garmin|           300|boutique paris|    physique|       île-de-france|2023-06-16|       5|         NULL|\n",
      "+--------------+----------+----------+------------+----------------+-----------------+--------------+--------------+--------------+------------+--------------------+----------+--------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#1 task. Normalize string columns (lowercase + remove extra spaces)\n",
    "#detecting which columns are string\n",
    "string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, T.StringType)]\n",
    "\n",
    "# for each string column, convert to lowercase and trim spaces\n",
    "for c in string_cols:\n",
    "    df = df.withColumn(c, F.lower(F.trim(F.col(c))))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce207a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+------------+-----------+-----------------+--------------+--------------+-----------+------------+--------------+----+--------+-------------+\n",
      "|id_transaction|client_nom|client_age|client_ville|produit_nom|produit_categorie|produit_marque|prix_catalogue|magasin_nom|magasin_type|magasin_region|date|quantite|montant_total|\n",
      "+--------------+----------+----------+------------+-----------+-----------------+--------------+--------------+-----------+------------+--------------+----+--------+-------------+\n",
      "+--------------+----------+----------+------------+-----------+-----------------+--------------+--------------+-----------+------------+--------------+----+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Replace empty strings with None (NULL) to detect missing values\n",
    "for c in df.columns:\n",
    "    df = df.withColumn(c, F.when(F.col(c) == \"\", None).otherwise(F.col(c)))\n",
    "\n",
    "df = df.na.drop(how=\"any\") #drop rows with any NULL values\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c9dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
